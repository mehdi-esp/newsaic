{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "iTbfbW6ilF0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp1F5KNUhDfu",
        "outputId": "c01a0b86-b5bd-4cca-fab4-66e3d4d0a595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.39.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.9)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=e3eea24190201b192b5f3305ed8d1fc5854230bedc5d942de275ed0c24ae0938\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n",
            "Setup complete. Libraries imported.\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Gemini Client initialized successfully.\n",
            "Using model: gemini-2.5-flash\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. Install necessary libraries (Run this cell once)\n",
        "!pip install google-genai pandas requests feedparser beautifulsoup4\n",
        "# 2. Import libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "from google import genai\n",
        "from google.genai.errors import APIError\n",
        "import os\n",
        "from google.colab import drive\n",
        "import requests\n",
        "try:\n",
        "    # Use Colab's specific method for reliable secret access\n",
        "    from google.colab import userdata\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments\n",
        "    userdata = None\n",
        "\n",
        "print(\"Setup complete. Libraries imported.\")\n",
        "\n",
        "# 3. Mount Google Drive (Essential for accessing the JSON file)\n",
        "# Force remount to avoid mounting issues\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "# 4. LLM API Key and Initialization (Loads key from Colab Secrets)\n",
        "GEMINI_API_KEY = None\n",
        "if userdata:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "else:\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found. Please set it in Colab Secrets (üîë).\")\n",
        "\n",
        "try:\n",
        "    # Initialize the Gemini Client\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "    print(\"Gemini Client initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Gemini client: {e}\")\n",
        "\n",
        "# Define the model to use for rewriting\n",
        "REWRITING_MODEL = 'gemini-2.5-flash'\n",
        "print(f\"Using model: {REWRITING_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Preparation"
      ],
      "metadata": {
        "id": "lAgI-zFBlEgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: Adjust this path if your file is in a different folder!\n",
        "FILE_NAME = '/content/drive/MyDrive/NewsBot_AI/fetched_articles.json'\n",
        "\n",
        "try:\n",
        "    articles_df = pd.read_json(FILE_NAME, orient='records')\n",
        "\n",
        "    # Simple cleanup: fill any potential empty body text to prevent LLM errors\n",
        "    articles_df['body_text_html'] = articles_df['body_text_html'].fillna('')\n",
        "\n",
        "    print(f\"\\nSuccessfully loaded {articles_df.shape[0]} articles from {FILE_NAME}\")\n",
        "\n",
        "    # Display the first article's details to confirm data integrity\n",
        "    sample_article = articles_df.iloc[0]\n",
        "    print(f\"Sample Article Title: {sample_article['title']}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nERROR: File '{FILE_NAME}' not found.\")\n",
        "    print(\"ACTION: Running fetch below to generate it.\")\n",
        "    articles_df = pd.DataFrame()\n",
        "\n",
        "# Fetch News via RSS Aggregation (Simulating FreshRSS: Curated feeds + full extraction/normalization)\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup  # For XPath-like extraction (e.g., //div[@class='content'])\n",
        "import json\n",
        "import pandas as pd  # Already imported\n",
        "import os\n",
        "from hashlib import md5  # For deduplication by title hash\n",
        "\n",
        "# Curated RSS feeds for technology news (full text/excerpts, per current sources)\n",
        "rss_feeds = [\n",
        "    \"https://www.techrepublic.com/feed/\",  # TechRepublic: Business tech news\n",
        "    \"https://www.cnet.com/rss/news/\",  # CNET: Tech reviews/news\n",
        "    \"https://feeds.bbci.co.uk/news/technology/rss.xml\"  # BBC Technology: Global tech coverage\n",
        "]\n",
        "\n",
        "raw_articles = []\n",
        "seen_titles = set()  # For deduplication\n",
        "\n",
        "for feed_url in rss_feeds:\n",
        "    try:\n",
        "        print(f\"Fetching from {feed_url}...\")\n",
        "        feed = feedparser.parse(feed_url)\n",
        "\n",
        "        for entry in feed.entries[:4]:  # Limit to 4 per feed for demo (total ~12, deduped)\n",
        "            title = entry.get('title', '').strip()\n",
        "            title_hash = md5(title.encode()).hexdigest()  # Normalize/dedup\n",
        "            if title_hash in seen_titles:\n",
        "                continue  # Skip duplicates\n",
        "            seen_titles.add(title_hash)\n",
        "\n",
        "            link = entry.get('link', '')\n",
        "            topic = 'technology'  # Default; refine from categories if available\n",
        "            author = entry.get('author', '')\n",
        "            pub_date = entry.get('published', '')\n",
        "\n",
        "            # Extract topic tags from tags/categories if present\n",
        "            topic_tags = [tag.term for tag in entry.get('tags', [])] or ['technology']\n",
        "\n",
        "            # Get full body: Use summary if available, else fetch from link (FreshRSS-style scraping)\n",
        "            body_html = entry.get('summary', '') or entry.get('content', [{}])[0].get('value', '')\n",
        "            if len(body_html) < 200 and link:  # If short, fetch full (respectful)\n",
        "                headers = {'User-Agent': 'Mozilla/5.0 (compatible; NewsBot Student Project)'}\n",
        "                resp = requests.get(link, headers=headers, timeout=10)\n",
        "                if resp.status_code == 200:\n",
        "                    soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "                    # XPath-like extraction: //article or //div[@class='content'] or //main\n",
        "                    content_div = soup.find('article') or soup.find('div', class_='content') or soup.find('main') or soup.find('div', id='article-body')\n",
        "                    if content_div:\n",
        "                        body_html = content_div.get_text(separator=' ', strip=True)\n",
        "                    else:\n",
        "                        body_html = resp.text[:3000]  # Fallback truncate\n",
        "                    # Clean: Remove extra whitespace, limit for LLM\n",
        "                    body_html = ' '.join(body_html.split())[:5000]\n",
        "\n",
        "            article = {\n",
        "                \"original_id\": f\"freshrss-{hash(title) % 1000}\",  # Unique ID\n",
        "                \"title\": title,\n",
        "                \"topic\": topic,\n",
        "                \"body_text_html\": body_html,  # Full extracted text\n",
        "                \"author\": author,\n",
        "                \"publication_date\": pub_date,\n",
        "                \"tags\": topic_tags[:5]  # Limit tags\n",
        "            }\n",
        "            raw_articles.append(article)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {feed_url}: {e}\")\n",
        "\n",
        "if raw_articles:\n",
        "    # Save to JSON (in your Google Drive path)\n",
        "    fetched_file = FILE_NAME\n",
        "    os.makedirs(os.path.dirname(fetched_file), exist_ok=True)\n",
        "    with open(fetched_file, 'w') as f:\n",
        "        json.dump(raw_articles, f, indent=4)\n",
        "\n",
        "    print(f\"Successfully fetched {len(raw_articles)} articles via RSS aggregation (FreshRSS-style) and saved to {fetched_file}\")\n",
        "\n",
        "    # Load into DataFrame\n",
        "    articles_df = pd.DataFrame(raw_articles)\n",
        "    articles_df['body_text_html'] = articles_df['body_text_html'].fillna('')\n",
        "\n",
        "    # Display sample\n",
        "    print(f\"Sample Title: {articles_df.iloc[0]['title'][:100]}...\")\n",
        "    print(f\"Sample Body Snippet: {articles_df.iloc[0]['body_text_html'][:150]}...\")\n",
        "else:\n",
        "    print(\"No articles fetched. Check internet or feeds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR8amtG1hSru",
        "outputId": "d5f5e85b-f7e8-48de-a8c4-e2df49e52369"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully loaded 10 articles from /content/drive/MyDrive/NewsBot_AI/fetched_articles.json\n",
            "Sample Article Title: Cybersecurity Starts With You: Lessons From Phishing, Ransomware, and Real-World Mistakes\n",
            "Fetching from https://www.techrepublic.com/feed/...\n",
            "Fetching from https://www.cnet.com/rss/news/...\n",
            "Fetching from https://feeds.bbci.co.uk/news/technology/rss.xml...\n",
            "Successfully fetched 12 articles via RSS aggregation (FreshRSS-style) and saved to /content/drive/MyDrive/NewsBot_AI/fetched_articles.json\n",
            "Sample Title: Cybersecurity Starts With You: Lessons From Phishing, Ransomware, and Real-World Mistakes...\n",
            "Sample Body Snippet: <p>This Cybersecurity Awareness Month, see how real-world phishing and ransomware attacks reveal why every employee plays a role in protection.</p>\n",
            "<p...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Persona definition"
      ],
      "metadata": {
        "id": "ROlb-SZYk9YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PERSONAS = {\n",
        "    \"Gen Z Trailblazer\": {\n",
        "        \"age\": 19,\n",
        "        \"tone\": \"Witty, meme-savvy, and packed with Gen Z flair (e.g., 'spill the tea,' 'no cap,' 'big mood,' 'vibes on fleek').\",\n",
        "        \"style_description\": \"Ultra-casual, bold, and dripping with pop culture references, like a TikTok live sesh.\",\n",
        "        \"length_target\": \"Keep it short, punchy, and scroll-friendly (max 120 words).\"\n",
        "    },\n",
        "    \"Corporate Visionary\": {\n",
        "        \"age\": 50,\n",
        "        \"tone\": \"Confident, strategic, with a dash of futurist jargon (e.g., 'disruptive innovation,' 'ecosystem leverage,' 'next-gen paradigm').\",\n",
        "        \"style_description\": \"Polished, forward-thinking, and tailored for C-suite briefings.\",\n",
        "        \"length_target\": \"Deliver a crisp executive insight (max 90 words).\"\n",
        "    },\n",
        "    \"Eco-Philosopher\": {\n",
        "        \"age\": 35,\n",
        "        \"tone\": \"Thoughtful, eco-conscious, with poetic undertones and ethical musings (e.g., 'harmony with nature,' 'sustainable ethos,' 'planetary balance').\",\n",
        "        \"style_description\": \"Reflective, nature-inspired, blending tech with environmental wisdom.\",\n",
        "        \"length_target\": \"Craft a meditative summary with depth (max 200 words).\"\n",
        "    },\n",
        "    \"Cyberpunk Dreamer\": {\n",
        "        \"age\": 28,\n",
        "        \"tone\": \"Edgy, dystopian, infused with neon-lit slang (e.g., 'data jack,' 'synth-life,' 'grid runner,' 'chrome hustle').\",\n",
        "        \"style_description\": \"Gritty, futuristic, like a cyberpunk novel excerpt.\",\n",
        "        \"length_target\": \"Keep it sleek and immersive (max 150 words).\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nDefined 4 creative User Personas for personalized rewriting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlhfKJtNhufs",
        "outputId": "aa1e3b89-289f-469a-b4ba-7bca0c743663"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defined 4 creative User Personas for personalized rewriting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The News Rewriting Function (LLM Engine)"
      ],
      "metadata": {
        "id": "5KGj-ur3k5rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite_article(article: pd.Series, persona_name: str) -> dict:\n",
        "    \"\"\"Constructs a detailed prompt and calls the Gemini API to rewrite the article.\"\"\"\n",
        "    persona = PERSONAS.get(persona_name)\n",
        "    if not persona:\n",
        "        return {\"error\": f\"Persona '{persona_name}' not found.\"}\n",
        "\n",
        "    # 1. System Prompt (High-level instruction for the model)\n",
        "    system_instruction = (\n",
        "        \"You are an expert content personalizer running on an AI News Platform. \"\n",
        "        \"Your task is to rewrite the provided news article to perfectly match the target user persona's style, tone, and length. \"\n",
        "        \"DO NOT use HTML tags. ONLY output the rewritten text.\"\n",
        "    )\n",
        "\n",
        "    # 2. User Prompt (The payload sent to the model)\n",
        "    user_prompt = f\"\"\"\n",
        "    --- REWRITING INSTRUCTIONS (PERSONA: {persona_name}) ---\n",
        "\n",
        "    * **TARGET STYLE:** Rewrite the content in a {persona['style_description']} style.\n",
        "    * **TONE:** The tone must be {persona['tone']}.\n",
        "    * **LENGTH:** {persona['length_target']}\n",
        "    * **AGE/DEMO:** Address a reader who is approximately {persona['age']} years old.\n",
        "\n",
        "    --- ORIGINAL ARTICLE CONTENT ---\n",
        "\n",
        "    Title: {article['title']}\n",
        "    Original Source Topic: {article['topic']}\n",
        "    Original Text:\n",
        "\n",
        "    {article['body_text_html']}\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"-> Rewriting '{article['title'][:50]}...' for persona: {persona_name}...\")\n",
        "\n",
        "    try:\n",
        "        # Call the Gemini API\n",
        "        response = client.models.generate_content(\n",
        "            model=REWRITING_MODEL,\n",
        "            contents=[user_prompt],\n",
        "            config=genai.types.GenerateContentConfig(\n",
        "                system_instruction=system_instruction\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Structure the output as a RewrittenArticle object\n",
        "        return {\n",
        "            \"title\": f\"[{persona_name} Rewritten] {article['title']}\",\n",
        "            \"content_text\": response.text,\n",
        "            \"original_article\": article['original_id'],\n",
        "            \"persona\": persona_name\n",
        "        }\n",
        "\n",
        "    except APIError as e:\n",
        "        return {\n",
        "            \"title\": \"API Error\",\n",
        "            \"content_text\": f\"API ERROR: Could not rewrite article. {e}\",\n",
        "            \"original_article\": article['original_id'],\n",
        "            \"persona\": persona_name\n",
        "        }\n",
        "    except Exception as e:\n",
        "         return {\n",
        "            \"title\": \"Error\",\n",
        "            \"content_text\": f\"An unexpected error occurred: {e}\",\n",
        "            \"original_article\": article['original_id'],\n",
        "            \"persona\": persona_name\n",
        "        }"
      ],
      "metadata": {
        "id": "dDRHZivAkbED"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demonstration"
      ],
      "metadata": {
        "id": "oEn3E6oMk2o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not articles_df.empty:\n",
        "    # 1. Select the first article for demonstration\n",
        "    article_to_rewrite = articles_df.iloc[0]\n",
        "\n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\"  DEMONSTRATION START\")\n",
        "    print(f\"  Source Article: {article_to_rewrite['title']}\")\n",
        "    print(f\"=======================================================\\n\")\n",
        "\n",
        "    # 2. Rewrite the article for the Gen Z Trailblazer Persona\n",
        "    genz_result = rewrite_article(article_to_rewrite, \"Gen Z Trailblazer\")\n",
        "\n",
        "    # 3. Rewrite the SAME article for the Corporate Visionary Persona\n",
        "    corp_result = rewrite_article(article_to_rewrite, \"Corporate Visionary\")\n",
        "\n",
        "    # 4. Rewrite for the Eco-Philosopher Persona\n",
        "    eco_result = rewrite_article(article_to_rewrite, \"Eco-Philosopher\")\n",
        "\n",
        "    # 5. Rewrite for the Cyberpunk Dreamer Persona\n",
        "    cyber_result = rewrite_article(article_to_rewrite, \"Cyberpunk Dreamer\")\n",
        "\n",
        "    # 6. Display the results clearly\n",
        "    print(\"\\n-------------------------------------------------------\")\n",
        "    print(f\"  ORIGINAL TEXT SNIPPET (First 150 chars)\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    print(f\"{article_to_rewrite['body_text_html'][:150]}...\\n\")\n",
        "\n",
        "    # Gen Z Trailblazer Output\n",
        "    print(f\"\\n--- Output for Persona: {genz_result['persona']} ---\")\n",
        "    print(genz_result['content_text'])\n",
        "\n",
        "    # Corporate Visionary Output\n",
        "    print(f\"\\n--- Output for Persona: {corp_result['persona']} ---\")\n",
        "    print(corp_result['content_text'])\n",
        "\n",
        "    # Eco-Philosopher Output\n",
        "    print(f\"\\n--- Output for Persona: {eco_result['persona']} ---\")\n",
        "    print(eco_result['content_text'])\n",
        "\n",
        "    # Cyberpunk Dreamer Output\n",
        "    print(f\"\\n--- Output for Persona: {cyber_result['persona']} ---\")\n",
        "    print(cyber_result['content_text'])\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping demonstration because no articles were loaded. Check Section 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsEnT6ZukkT5",
        "outputId": "10e194dc-f23d-486c-89c1-94dc4f035ecb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "  DEMONSTRATION START\n",
            "  Source Article: Cybersecurity Starts With You: Lessons From Phishing, Ransomware, and Real-World Mistakes\n",
            "=======================================================\n",
            "\n",
            "-> Rewriting 'Cybersecurity Starts With You: Lessons From Phishi...' for persona: Gen Z Trailblazer...\n",
            "-> Rewriting 'Cybersecurity Starts With You: Lessons From Phishi...' for persona: Corporate Visionary...\n",
            "-> Rewriting 'Cybersecurity Starts With You: Lessons From Phishi...' for persona: Eco-Philosopher...\n",
            "-> Rewriting 'Cybersecurity Starts With You: Lessons From Phishi...' for persona: Cyberpunk Dreamer...\n",
            "\n",
            "-------------------------------------------------------\n",
            "  ORIGINAL TEXT SNIPPET (First 150 chars)\n",
            "-------------------------------------------------------\n",
            "<p>This Cybersecurity Awareness Month, see how real-world phishing and ransomware attacks reveal why every employee plays a role in protection.</p>\n",
            "<p...\n",
            "\n",
            "\n",
            "--- Output for Persona: Gen Z Trailblazer ---\n",
            "Okay, besties, spill the tea: it's Cybersecurity Awareness Month, and we need to talk. Ever heard of phishing or ransomware? Yeah, those are *not* the vibes. Real talk: your clicks and choices legit make or break the whole digital security game for everyone.\n",
            "\n",
            "No cap, these real-world fails show why *you* are the main character in keeping things safe. Don't be the reason for a major digital L! Stay woke, stay secure. Your main character energy needs protecting from the sus stuff online. Big mood: keep those passwords strong and that info locked down!\n",
            "\n",
            "--- Output for Persona: Corporate Visionary ---\n",
            "Elevating our enterprise's digital resilience is paramount. In a landscape dominated by sophisticated phishing and ransomware, a robust, next-gen security paradigm hinges on pervasive human-centric defenses. Empowering every team member as a proactive guardian isn't merely an operational mandate; it's a strategic imperative for ecosystem integrity and sustained innovation. This month underscores the critical individual contribution to our collective cyber posture, ensuring we mitigate risks and protect our core assets in this disruptive era.\n",
            "\n",
            "--- Output for Persona: Eco-Philosopher ---\n",
            "As the digital winds whisper through our interconnected world, this Cybersecurity Awareness Month invites us to pause and reflect. Much like the delicate balance of an ancient forest, our digital ecosystems thrive on the strength of every leaf, every root, every living entity. Phishing and ransomware, these invasive currents in our shared digital stream, are not mere technical glitches; they are symptoms of a deeper disconnect, a forgetting of our inherent responsibility as stewards of the virtual realm.\n",
            "\n",
            "Each click, each shared piece of information, is an act of digital gardening. To cultivate a truly resilient and harmonious online existence, we must embrace a sustainable ethos of awareness. Just as a single act of waste can pollute a pristine river, a moment of inattention can compromise the entire digital commons. Our individual vigilance is the fertile ground from which collective protection blossoms, ensuring planetary balance within the digital landscape. Let us tend to our digital garden with thoughtful intention, for true security, like true sustainability, begins within.\n",
            "\n",
            "--- Output for Persona: Cyberpunk Dreamer ---\n",
            "Listen up, chrome-cowboys and synth-grinders. This cycle, the Net-Awareness directive screams a truth etched in data-blood: your digital skin ain't impenetrable. Real-world phish-hooks, slithering through data-jacks, and ransomware gangs caging corporate minds reveal a stark reality. Every single one of you, from the low-level data-slave to the highest-tier grid-runner, is a vital node in the chrome hustle. One twitch, one wrong click, and the entire system bleeds. Guard your intel, keep your cyber-sense sharp. Your survival, and the corporate entity's, depends on it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ePg9rFQkouH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}