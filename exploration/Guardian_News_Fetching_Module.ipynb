{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup and API Authentication"
      ],
      "metadata": {
        "id": "6yN9XtqAyFgT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH3Hq026x8p-",
        "outputId": "81fa4283-ea35-4af8-96a0-ec85057338af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Libraries imported.\n",
            "API Key successfully loaded from environment and BASE_URL defined.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import sqlite3\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- Colab Secret Access ---\n",
        "# Use the recommended Colab method to reliably retrieve the secret\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GUARDIAN_API_KEY = userdata.get('GUARDIAN_API_KEY')\n",
        "except ImportError:\n",
        "    # Fallback for local environments or different notebook environments\n",
        "    GUARDIAN_API_KEY = os.getenv('GUARDIAN_API_KEY')\n",
        "\n",
        "if not GUARDIAN_API_KEY:\n",
        "    raise ValueError(\"GUARDIAN_API_KEY environment variable not found. Please set it securely via Colab Secrets.\")\n",
        "\n",
        "BASE_URL = \"https://content.guardianapis.com/search\"\n",
        "DB_NAME = 'news_cache.db'\n",
        "\n",
        "print(\"Setup complete. Libraries imported.\")\n",
        "print(\"API Key successfully loaded from environment and BASE_URL defined.\")\n",
        "\n",
        "# 1. Initialize SQLite Database\n",
        "def initialize_db():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS articles (\n",
        "        original_id TEXT PRIMARY KEY,\n",
        "        title TEXT,\n",
        "        author_name TEXT,\n",
        "        publication_date TEXT,\n",
        "        topic TEXT,\n",
        "        url TEXT,\n",
        "        body_text_html TEXT,\n",
        "        fetch_timestamp TEXT\n",
        "    );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"SQLite database '{DB_NAME}' initialized with 'articles' table.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FFetch and Normalization Functions (Local DB Management)"
      ],
      "metadata": {
        "id": "4qb5NB4hyKeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_save_article(article_data: dict, conn: sqlite3.Connection) -> dict:\n",
        "    \"\"\"Processes a single Guardian article JSON dict and saves it to the database.\"\"\"\n",
        "    cursor = conn.cursor()\n",
        "    fields = article_data.get('fields', {})\n",
        "\n",
        "    # Data Extraction\n",
        "    author_tag = next((tag for tag in article_data.get('tags', []) if tag.get('type') == 'contributor'), None)\n",
        "    author_name = author_tag['webTitle'] if author_tag else 'Unknown Persona'\n",
        "    body_content = fields.get('body', fields.get('trailText', ''))\n",
        "\n",
        "    if len(body_content) < 100:\n",
        "         return None\n",
        "\n",
        "    # Normalized structure\n",
        "    normalized_article = {\n",
        "        'original_id': article_data.get('id'),\n",
        "        'title': article_data.get('webTitle'),\n",
        "        'author_name': author_name,\n",
        "        'publication_date': article_data.get('webPublicationDate'),\n",
        "        'topic': article_data.get('sectionName'),\n",
        "        'url': article_data.get('webUrl'),\n",
        "        'body_text_html': body_content,\n",
        "        'fetch_timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Insert/Replace article\n",
        "    cursor.execute(\"\"\"\n",
        "    INSERT OR REPLACE INTO articles (original_id, title, author_name, publication_date, topic, url, body_text_html, fetch_timestamp)\n",
        "    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", tuple(normalized_article.values()))\n",
        "\n",
        "    return normalized_article\n",
        "\n",
        "\n",
        "def fetch_articles(topic: str, count: int = 10, days_ago: int = 1) -> list:\n",
        "    \"\"\"Fetches articles from the Guardian API, saving them using a local connection.\"\"\"\n",
        "\n",
        "    # 1. ESTABLISH LOCAL CONNECTION\n",
        "    local_conn = sqlite3.connect(DB_NAME)\n",
        "\n",
        "    from_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
        "    page_size = min(count, 50)\n",
        "\n",
        "    params = {\n",
        "        'api-key': GUARDIAN_API_KEY,\n",
        "        'q': topic,\n",
        "        'order-by': 'newest',\n",
        "        'page-size': page_size,\n",
        "        'show-fields': 'all',\n",
        "        'show-tags': 'contributor',\n",
        "        'from-date': from_date,\n",
        "        'type': 'article'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"ERROR: API request failed for topic '{topic}': {e}\")\n",
        "        local_conn.close()\n",
        "        return []\n",
        "\n",
        "    if data.get('response', {}).get('status') != 'ok':\n",
        "        print(f\"ERROR: API status not OK for topic '{topic}'. Response: {data}\")\n",
        "        local_conn.close()\n",
        "        return []\n",
        "\n",
        "    # Process and save new articles\n",
        "    fetched_list = []\n",
        "    for article in data['response']['results']:\n",
        "        normalized = normalize_and_save_article(article, local_conn)\n",
        "        if normalized:\n",
        "            fetched_list.append(normalized)\n",
        "\n",
        "    local_conn.commit()\n",
        "    local_conn.close()\n",
        "\n",
        "    print(f\"--- SUCCESS: Fetched and saved {len(fetched_list)} articles for topic: {topic} ---\")\n",
        "    return fetched_list"
      ],
      "metadata": {
        "id": "Sy9rw8d8y_wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cashing test function"
      ],
      "metadata": {
        "id": "c6TBb_5t8_sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_or_fetch_articles(topic: str, count: int = 10) -> list:\n",
        "    \"\"\"\n",
        "    Implements caching logic: retrieves articles from DB cache if available and fresh,\n",
        "    otherwise calls the API fetch function.\n",
        "    \"\"\"\n",
        "    CACHE_EXPIRY_HOURS = 1\n",
        "\n",
        "    print(f\"\\n--- Processing request for topic: {topic} ---\")\n",
        "\n",
        "    # 1. Try to retrieve articles from the DB cache first\n",
        "    expiry_time = (datetime.now() - timedelta(hours=CACHE_EXPIRY_HOURS)).isoformat()\n",
        "\n",
        "    # Open local connection for checking cache\n",
        "    conn_check = sqlite3.connect(DB_NAME)\n",
        "    cursor_check = conn_check.cursor()\n",
        "\n",
        "    # We will use the simplified, functional cache check logic for reliable demonstration\n",
        "    cursor_check.execute(\"\"\"\n",
        "    SELECT title, body_text_html\n",
        "    FROM articles\n",
        "    WHERE fetch_timestamp > ?\n",
        "    LIMIT ?\n",
        "    \"\"\", (expiry_time, count))\n",
        "\n",
        "    cached_results = cursor_check.fetchall()\n",
        "    conn_check.close() # Close connection used just for checking\n",
        "\n",
        "    if len(cached_results) >= count:\n",
        "        print(f\"✅ CACHE HIT: Found {len(cached_results)} fresh articles in the local database (cache). Skipping API call.\")\n",
        "        return [{\"title\": r[0], \"body_text_html\": r[1], \"source\": \"CACHE\"} for r in cached_results]\n",
        "\n",
        "    # 2. Cache Miss: Must call the external API\n",
        "    print(f\"❌ CACHE MISS: Only found {len(cached_results)} articles or cache expired. Calling Guardian API...\")\n",
        "\n",
        "    # Call the actual fetch function (this costs 1 API request and saves to the DB)\n",
        "    api_articles = fetch_articles(topic=topic, count=count)\n",
        "\n",
        "    # Ensure the fetched articles are returned\n",
        "    return api_articles\n"
      ],
      "metadata": {
        "id": "QUgeOht68-8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execution and Caching Demonstration"
      ],
      "metadata": {
        "id": "C0QJzsvYyNRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC_A = 'AI'\n",
        "TOPIC_B = 'Space'\n",
        "NUM_ARTICLES = 3\n",
        "\n",
        "print(\"\\n\\n=============== CACHING DEMONSTRATION START ===============\")\n",
        "\n",
        "# 1. First run (Guaranteed Cache MISS or first fetch)\n",
        "run1_articles = retrieve_or_fetch_articles(TOPIC_A, NUM_ARTICLES)\n",
        "\n",
        "# 2. Second run for the SAME TOPIC immediately (Guaranteed Cache HIT)\n",
        "time.sleep(1)\n",
        "run2_articles = retrieve_or_fetch_articles(TOPIC_A, NUM_ARTICLES)\n",
        "\n",
        "# 3. Third run for a DIFFERENT TOPIC (Requires new fetch, unless cache is populated)\n",
        "run3_articles = retrieve_or_fetch_articles(TOPIC_B, NUM_ARTICLES)\n",
        "\n",
        "print(\"\\n=============== CACHING DEMONSTRATION END ===============\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNb7mvKXzEoh",
        "outputId": "971d4fba-9bcf-433d-af47-20e08f7480bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=============== CACHING DEMONSTRATION START ===============\n",
            "\n",
            "--- Processing request for topic: AI ---\n",
            "✅ CACHE HIT: Found 3 fresh articles in the local database (cache). Skipping API call.\n",
            "\n",
            "--- Processing request for topic: AI ---\n",
            "✅ CACHE HIT: Found 3 fresh articles in the local database (cache). Skipping API call.\n",
            "\n",
            "--- Processing request for topic: Space ---\n",
            "✅ CACHE HIT: Found 3 fresh articles in the local database (cache). Skipping API call.\n",
            "\n",
            "=============== CACHING DEMONSTRATION END ===============\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Data Extraction and Preparation"
      ],
      "metadata": {
        "id": "gHl98EiyyPYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Retrieve all articles from the database (Opening connection ONLY for this final read)\n",
        "conn_final = sqlite3.connect(DB_NAME)\n",
        "cursor_final = conn_final.cursor()\n",
        "\n",
        "cursor_final.execute(\"SELECT original_id, title, author_name, publication_date, topic, url, body_text_html FROM articles\")\n",
        "db_data = cursor_final.fetchall()\n",
        "\n",
        "# 2. Close the connection immediately after fetching data\n",
        "conn_final.close()\n",
        "\n",
        "# 3. Convert to a Pandas DataFrame for the next notebook\n",
        "df_columns = ['original_id', 'title', 'author_name', 'publication_date', 'topic', 'url', 'body_text_html']\n",
        "final_articles_df = pd.DataFrame(db_data, columns=df_columns)\n",
        "\n",
        "print(\"\\n\\n--- FINAL DATA EXTRACTION ---\")\n",
        "print(f\"Total Unique Articles Ready for Rewriting: {final_articles_df.shape[0]}\")\n",
        "print(final_articles_df[['title', 'topic', 'author_name']].head())\n",
        "\n",
        "# 4. Save the data to a JSON file\n",
        "final_articles_df.to_json('raw_articles_for_rewriting.json', orient='records', indent=4)\n",
        "print(\"\\nData saved to 'raw_articles_for_rewriting.json'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIY9J1xQzfkC",
        "outputId": "02cec102-4b12-4e01-e6f3-bf1a1a23428e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- FINAL DATA EXTRACTION ---\n",
            "Total Unique Articles Ready for Rewriting: 5\n",
            "                                               title           topic  \\\n",
            "0  IMF chief warns ‘uncertainty is the new normal...        Business   \n",
            "1  Katy Perry review – ​like being high on Haribo...           Music   \n",
            "2  ‘Rawdogging’ marathons: has gen Z discovered t...         Society   \n",
            "3  A drag queen stands at a site of violence: Lee...  Art and design   \n",
            "4  The Life of a Showgirl is a massive hit – and ...           Music   \n",
            "\n",
            "        author_name  \n",
            "0   Heather Stewart  \n",
            "1    Claire Biddles  \n",
            "2   Unknown Persona  \n",
            "3  Charlotte Jansen  \n",
            "4     Shaad D'Souza  \n",
            "\n",
            "Data saved to 'raw_articles_for_rewriting.json'.\n"
          ]
        }
      ]
    }
  ]
}